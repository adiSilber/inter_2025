{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a0ed9c",
   "metadata": {},
   "source": [
    "# Inspect saved experiment datapoints and activations\n",
    "This notebook loads a saved experiment pickle (matching `math500*`) and prints summaries of datapoint text fields and captured activations. It is written to be robust to missing attributes and different activation tensor types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6097132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pickle, sys\n",
    "from pprint import pprint\n",
    "\n",
    "# Optional: prefer torch if available for tensor shape introspection\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception:\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    NUMPY_AVAILABLE = True\n",
    "except Exception:\n",
    "    NUMPY_AVAILABLE = False\n",
    "\n",
    "def _shape_of_item(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if TORCH_AVAILABLE and isinstance(x, torch.Tensor):\n",
    "        return tuple(x.shape)\n",
    "    if NUMPY_AVAILABLE and isinstance(x, np.ndarray):\n",
    "        return x.shape\n",
    "    # common Python containers with first element shapes\n",
    "    if isinstance(x, (list, tuple)) and len(x) > 0:\n",
    "        # attempt to describe first few entries\n",
    "        shapes = []\n",
    "        for v in x[:3]:\n",
    "            shapes.append(_shape_of_item(v))\n",
    "        return shapes\n",
    "    return type(x).__name__\n",
    "\n",
    "def summarize_activation_dict(act_dict, max_preview=3):\n",
    "    if act_dict is None:\n",
    "        print('    <None>')\n",
    "        return 0\n",
    "    if not isinstance(act_dict, dict):\n",
    "        print('    <not-a-dict>', type(act_dict))\n",
    "        return 0\n",
    "    total = 0\n",
    "    for k, lst in act_dict.items():\n",
    "        if lst is None:\n",
    "            print(f'    - {k}: None')\n",
    "            continue\n",
    "        try:\n",
    "            length = len(lst)\n",
    "        except Exception:\n",
    "            print(f'    - {k}: <not-a-list> {type(lst)}')\n",
    "            continue\n",
    "        total += length\n",
    "        preview = []\n",
    "        seen = 0\n",
    "        for item in lst:\n",
    "            if item is None:\n",
    "                preview.append(None)\n",
    "            else:\n",
    "                preview.append(_shape_of_item(item))\n",
    "            seen += 1\n",
    "            if seen >= max_preview:\n",
    "                break\n",
    "        print(f'    - {k}: {length} entries, preview shapes/types: {preview}')\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb0018b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.load failed, falling back to pickle.load: Invalid magic number; corrupt file?\n",
      "Loaded list of datapoints, count = 10\n"
     ]
    }
   ],
   "source": [
    "# Minimal loader: load the latest `math500*.pkl` and expect a list of datapoints\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Hide GPUs from this Python process so deserialization can't allocate to CUDA\n",
    "os.environ.setdefault('CUDA_VISIBLE_DEVICES', '')\n",
    "\n",
    "repo_root = os.path.abspath(os.getcwd())\n",
    "path = os.path.join(repo_root, 'math500_no_injection_20260119_174803.pkl')\n",
    "\n",
    "obj = None\n",
    "# Prefer torch.load with map_location='cpu' and weights_only=False to avoid allocating tensors on CUDA\n",
    "try:\n",
    "    import torch\n",
    "    try:\n",
    "        obj = torch.load(path, map_location='cpu', weights_only=False)\n",
    "        print('Loaded with torch.load(map_location=\"cpu\", weights_only=False)')\n",
    "    except Exception as e:\n",
    "        print('torch.load failed, falling back to pickle.load:', e)\n",
    "        with open(path, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "except Exception as e:\n",
    "    # torch not available or import failed; fallback to pickle\n",
    "    print('torch unavailable or errored, using pickle.load:', e)\n",
    "    with open(path, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "# Prefer a plain list of datapoints; if the pickle contains an Experiment-like\n",
    "# object with a `.datapoints` attribute, extract that list.\n",
    "if isinstance(obj, list):\n",
    "    datapoints = obj\n",
    "    print('Loaded list of datapoints, count =', len(datapoints))\n",
    "elif hasattr(obj, 'datapoints'):\n",
    "    datapoints = obj.datapoints\n",
    "    print('Loaded object with .datapoints, count =', len(datapoints))\n",
    "else:\n",
    "    raise TypeError('Pickle did not contain a list of datapoints or an object with `.datapoints`')\n",
    "\n",
    "# Expose datapoints in the notebook namespace\n",
    "globals()['datapoints'] = datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238fb3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['layer_0_attention', 'layer_1_attention', 'layer_2_attention', 'layer_3_attention', 'layer_4_attention', 'layer_5_attention', 'layer_6_attention', 'layer_7_attention', 'layer_8_attention', 'layer_9_attention', 'layer_10_attention', 'layer_11_attention', 'layer_12_attention', 'layer_13_attention', 'layer_14_attention', 'layer_15_attention', 'layer_16_attention', 'layer_17_attention', 'layer_18_attention', 'layer_19_attention', 'layer_20_attention', 'layer_21_attention', 'layer_22_attention', 'layer_23_attention', 'layer_24_attention', 'layer_25_attention', 'layer_26_attention', 'layer_27_attention'])\n"
     ]
    }
   ],
   "source": [
    "print(datapoints[0].activations_upto_injection.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f49837",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_attention = datapoints[0].question_contents[\"layer_1_attention\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a022a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inter2025_vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
