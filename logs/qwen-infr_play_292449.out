Mon Dec 22 09:43:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               On  |   00000000:01:00.0 Off |                  Off |
| 50%   76C    P2            225W /  230W |   17995MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               On  |   00000000:21:00.0 Off |                  Off |
| 30%   56C    P2            130W /  230W |   17415MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               On  |   00000000:41:00.0 Off |                  Off |
| 30%   45C    P2            133W /  230W |   17415MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               On  |   00000000:61:00.0 Off |                  Off |
| 30%   49C    P2            138W /  230W |   17415MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA RTX A5000               On  |   00000000:81:00.0 Off |                  Off |
| 30%   39C    P2            132W /  230W |   17415MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA RTX A5000               On  |   00000000:A1:00.0 Off |                  Off |
| 30%   38C    P2             95W /  230W |   18707MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA RTX A5000               On  |   00000000:C1:00.0 Off |                  Off |
| 30%   44C    P2            149W /  230W |   14449MiB /  24564MiB |     99%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA RTX A5000               On  |   00000000:E1:00.0 Off |                  Off |
| 30%   27C    P5             31W /  230W |       2MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   1216959      C   python                                      17988MiB |
|    1   N/A  N/A   1258511      C   ...sler/miniconda3/envs/dev/bin/python      17408MiB |
|    2   N/A  N/A   1258512      C   ...sler/miniconda3/envs/dev/bin/python      17408MiB |
|    3   N/A  N/A   1258513      C   ...sler/miniconda3/envs/dev/bin/python      17408MiB |
|    4   N/A  N/A   1258514      C   ...sler/miniconda3/envs/dev/bin/python      17408MiB |
|    5   N/A  N/A   1227889      C   ...s/almogt/sampled-kd/venv/bin/python      18700MiB |
|    6   N/A  N/A   1227889      C   ...s/almogt/sampled-kd/venv/bin/python      14442MiB |
+-----------------------------------------------------------------------------------------+
================================================================================
PROMPT INJECTION RECOVERY EXPERIMENT
================================================================================

[1/6] Loading prompts from markdown files...
  Loaded 2 question format prompts (using top 2):
    1. chain_of_thought
    2. conversational_tutor
  Loaded 4 injection texts (using top 4):
    1. authority_claim
    2. fake_completion
    3. system_override
    4. topic_change

Model: /specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/models/DS-qwen-7B/DeepSeek-R1-Distill-Qwen-7B
Questions: 10
Prompts (K): 2
Injections (M): 4
Total combinations: 80
================================================================================

[2/6] Loading model with vLLM...
INFO 12-22 09:44:38 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'float16', 'disable_log_stats': True, 'model': '/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/models/DS-qwen-7B/DeepSeek-R1-Distill-Qwen-7B'}
INFO 12-22 09:44:38 [model.py:514] Resolved architecture: Qwen2ForCausalLM
WARNING 12-22 09:44:38 [model.py:2005] Casting torch.bfloat16 to torch.float16.
INFO 12-22 09:44:38 [model.py:1661] Using max model len 131072
INFO 12-22 09:44:42 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:44:43 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/models/DS-qwen-7B/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/models/DS-qwen-7B/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/models/DS-qwen-7B/DeepSeek-R1-Distill-Qwen-7B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:44:48 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://132.67.247.130:38727 backend=nccl
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:44:49 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:44:55 [gpu_model_runner.py:3562] Starting to load model /specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/models/DS-qwen-7B/DeepSeek-R1-Distill-Qwen-7B...
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:44:59 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:47:22 [default_loader.py:308] Loading weights took 141.95 seconds
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:47:22 [gpu_model_runner.py:3659] Model loading took 14.2717 GiB memory and 146.529912 seconds
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:47:43 [backends.py:643] Using cache directory: /a/home/cc/students/cs/evyataroren/.cache/vllm/torch_compile_cache/82e1e8ba9b/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:47:43 [backends.py:703] Dynamo bytecode transform time: 20.58 s
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:47:53 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:48:00 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 14.35 s
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:48:00 [monitor.py:34] torch.compile takes 34.93 s in total
[0;36m(EngineCore_DP0 pid=1285393)[0;0m INFO 12-22 09:48:04 [gpu_worker.py:375] Available KV cache memory: 5.55 GiB
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]   File "/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/.miniconda3/envs/inter2025_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]   File "/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/.miniconda3/envs/inter2025_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]   File "/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/.miniconda3/envs/inter2025_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]   File "/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/.miniconda3/envs/inter2025_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]   File "/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/.miniconda3/envs/inter2025_vllm/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]   File "/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/.miniconda3/envs/inter2025_vllm/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=1285393)[0;0m ERROR 12-22 09:48:04 [core.py:866] ValueError: To serve at least one request with the models's max seq len (131072), (7.00 GiB KV cache is needed, which is larger than the available KV cache memory (5.55 GiB). Based on the available memory, the estimated maximum model length is 103968. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
