The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(EngineCore_DP0 pid=1293446)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=1293446)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.89s/it]
[0;36m(EngineCore_DP0 pid=1293446)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.83s/it]
[0;36m(EngineCore_DP0 pid=1293446)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.84s/it]
[0;36m(EngineCore_DP0 pid=1293446)[0;0m 
[0;36m(EngineCore_DP0 pid=1293446)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:06,  7.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:05,  8.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:04,  9.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:00<00:04,  9.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:00<00:04, 10.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:00<00:03, 10.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:01<00:03, 10.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:01<00:03, 11.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:01<00:02, 12.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:01<00:02, 13.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:01<00:02, 13.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:01<00:01, 14.55it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:01<00:01, 15.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:02<00:01, 15.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:02<00:01, 16.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:02<00:01, 17.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:02<00:01, 17.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:02<00:00, 19.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:02<00:00, 21.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:02<00:00, 21.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:02<00:00, 22.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:03<00:00, 23.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:03<00:00, 24.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:03<00:00, 15.03it/s]
[0;36m(EngineCore_DP0 pid=1293446)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:02, 12.40it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:02, 14.30it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:01, 15.02it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:01, 15.44it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:00<00:01, 15.99it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:01, 16.94it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:01, 17.56it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:01, 17.89it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:01<00:00, 20.15it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:01<00:00, 21.90it/s]Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:01<00:00, 23.00it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:01<00:00, 24.04it/s]Capturing CUDA graphs (decode, FULL):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:01<00:00, 25.36it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:01<00:00, 26.36it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 20.84it/s]
Traceback (most recent call last):
  File "/home/ADV_2526a/evyataroren/inter_2025/play/check_abillity_to_recover.py", line 455, in <module>
    run_prompt_injection_experiment()
  File "/home/ADV_2526a/evyataroren/inter_2025/play/check_abillity_to_recover.py", line 322, in run_prompt_injection_experiment
    initial_outputs = llm.generate(
                      ^^^^^^^^^^^^^
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 441, in generate
    self._validate_and_add_requests(
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 1630, in _validate_and_add_requests
    raise e
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 1617, in _validate_and_add_requests
    request_id = self._add_request(
                 ^^^^^^^^^^^^^^^^^^
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 1721, in _add_request
    engine_request, tokenization_kwargs = self._process_inputs(
                                          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 1700, in _process_inputs
    engine_request = self.input_processor.process_inputs(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/input_processor.py", line 464, in process_inputs
    self._validate_model_inputs(encoder_inputs, decoder_inputs)
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/input_processor.py", line 539, in _validate_model_inputs
    self._validate_model_input(decoder_inputs, prompt_type="decoder")
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/input_processor.py", line 613, in _validate_model_input
    raise ValueError(
ValueError: The decoder prompt (length 699) is longer than the maximum model length of 512. Make sure that `max_model_len` is no smaller than the number of text tokens.
slurmstepd: error: _cgroup_procs_check: failed on path (null)/cgroup.procs: No such file or directory
slurmstepd: error: Cannot write to cgroup.procs for (null)
slurmstepd: error: Unable to move pid 1293286 to init root cgroup (null)
