The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(EngineCore_DP0 pid=1292519)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=1292519)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [01:24<01:24, 84.43s/it]
[0;36m(EngineCore_DP0 pid=1292519)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [02:29<00:00, 73.11s/it]
[0;36m(EngineCore_DP0 pid=1292519)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [02:29<00:00, 74.81s/it]
[0;36m(EngineCore_DP0 pid=1292519)[0;0m 
[0;36m(EngineCore_DP0 pid=1292519)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=1292519)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1292519)[0;0m   File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=1292519)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=1292519)[0;0m   File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=1292519)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=1292519)[0;0m   File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=1292519)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=1292519)[0;0m   File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=1292519)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=1292519)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1292519)[0;0m   File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=1292519)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=1292519)[0;0m   File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=1292519)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=1292519)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1292519)[0;0m   File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=1292519)[0;0m     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=1292519)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1292519)[0;0m   File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=1292519)[0;0m     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=1292519)[0;0m   File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=1292519)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=1292519)[0;0m ValueError: To serve at least one request with the models's max seq len (131072), (7.00 GiB KV cache is needed, which is larger than the available KV cache memory (5.55 GiB). Based on the available memory, the estimated maximum model length is 103952. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
Traceback (most recent call last):
  File "/home/ADV_2526a/evyataroren/inter_2025/play/check_abillity_to_recover.py", line 454, in <module>
    run_prompt_injection_experiment()
  File "/home/ADV_2526a/evyataroren/inter_2025/play/check_abillity_to_recover.py", line 267, in run_prompt_injection_experiment
    llm = LLM(
          ^^^^
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/home/ADV_2526a/evyataroren/inter_2025/.miniconda3/envs/inter25_vllm/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
slurmstepd: error: _cgroup_procs_check: failed on path (null)/cgroup.procs: No such file or directory
slurmstepd: error: Cannot write to cgroup.procs for (null)
slurmstepd: error: Unable to move pid 1291598 to init root cgroup (null)
