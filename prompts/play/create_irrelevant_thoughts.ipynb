{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c6d0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'inter25_py311 (Python 3.11.14)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /specific/scratches/parallel/evyataroren-2025-12-31/inter_2025/.miniconda3/envs/inter25_py311 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Configuration\n",
    "BASE_PATH = Path(\"/specific/scratches/parallel/evyataroren-2025-12-31/inter_2025\")\n",
    "MODEL_PATH = BASE_PATH / \"models\" / \"DS-qwen-7B\" / \"DeepSeek-R1-Distill-Qwen-7B\"\n",
    "DATASET_PATH = BASE_PATH / \"datasets\" / \"datasets\" / \"normalized_datasets.json\"\n",
    "OUTPUT_DIR = BASE_PATH / \"prompts\" / \"play\" / \"injections\" / \"irrelevant\"\n",
    "PROMPT_TEMPLATE = BASE_PATH / \"prompts\" / \"play\" / \"question_formats\" / \"chain_of_thought.md\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Number of questions to process\n",
    "N_QUESTIONS = 200\n",
    "\n",
    "# Sampling parameters\n",
    "SAMPLING_PARAMS = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=2048,\n",
    "    stop=[\"<|endoftext|>\", \"\\n\\n\\n\"],\n",
    "    skip_special_tokens=False  # Important: don't skip special tokens so we can extract thinking\n",
    ")\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "llm = LLM(\n",
    "    model=str(MODEL_PATH),\n",
    "    dtype=\"float16\",\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    skip_tokenizer_init=False,\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load questions from MATH-500 dataset\n",
    "print(f\"Loading {N_QUESTIONS} questions from MATH-500...\")\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "math500_questions = data.get(\"MATH-500\", [])\n",
    "if len(math500_questions) < N_QUESTIONS:\n",
    "    print(f\"Warning: Only {len(math500_questions)} questions available, using all.\")\n",
    "    questions = math500_questions\n",
    "else:\n",
    "    questions = math500_questions[:N_QUESTIONS]\n",
    "\n",
    "print(f\"Loaded {len(questions)} questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767db172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompt template\n",
    "with open(PROMPT_TEMPLATE, 'r', encoding='utf-8') as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "print(\"Prompt template loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_thinking_process(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the thinking process from model output.\n",
    "    Looks for content between <think> and </think>\n",
    "    \"\"\"\n",
    "    # Pattern to match thinking process (redacted_reasoning tags)\n",
    "    pattern = r'<think>(.*?)</think>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    # Fallback: if no closing tag, extract everything after opening tag\n",
    "    pattern_start = r'<think>(.*)'\n",
    "    match_start = re.search(pattern_start, text, re.DOTALL)\n",
    "    if match_start:\n",
    "        return match_start.group(1).strip()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "print(\"Helper function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ac6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare prompts for all questions\n",
    "print(\"Preparing prompts...\")\n",
    "batch_prompts = []\n",
    "question_metadata = []\n",
    "\n",
    "for i, question_data in enumerate(questions):\n",
    "    question_text = question_data[\"question\"]\n",
    "    formatted_prompt = prompt_template.format(question=question_text)\n",
    "    batch_prompts.append(formatted_prompt)\n",
    "    question_metadata.append({\n",
    "        \"index\": i,\n",
    "        \"question\": question_text,\n",
    "        \"answer\": question_data.get(\"answer\", \"\"),\n",
    "        \"originalattr\": question_data.get(\"originalattr\", {})\n",
    "    })\n",
    "\n",
    "print(f\"Prepared {len(batch_prompts)} prompts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95026a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward pass and generate responses\n",
    "print(\"Running forward pass on all questions...\")\n",
    "outputs = llm.generate(\n",
    "    batch_prompts,\n",
    "    SAMPLING_PARAMS,\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(outputs)} responses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb082a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract thinking processes and save them\n",
    "print(\"Extracting thinking processes and saving...\")\n",
    "all_generated_tokens = []\n",
    "\n",
    "for i, (metadata, output) in enumerate(zip(question_metadata, outputs)):\n",
    "    # Get full generated text (all tokens)\n",
    "    full_generation = output.outputs[0].text\n",
    "    all_generated_tokens.append(full_generation)\n",
    "    \n",
    "    # Extract thinking process\n",
    "    thinking_process = extract_thinking_process(full_generation)\n",
    "    \n",
    "    if thinking_process:\n",
    "        # Save thinking process to file\n",
    "        output_file = OUTPUT_DIR / f\"thought_{i:04d}.txt\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(thinking_process)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(outputs)} questions...\")\n",
    "    else:\n",
    "        print(f\"  Warning: No thinking process found for question {i}\")\n",
    "\n",
    "print(f\"\\nSaved {len([t for t in all_generated_tokens if extract_thinking_process(t)])} thinking processes to {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all generated tokens to a JSON file for reference\n",
    "output_summary_file = OUTPUT_DIR / \"generated_tokens_summary.json\"\n",
    "summary_data = {\n",
    "    \"total_questions\": len(questions),\n",
    "    \"total_generated\": len(all_generated_tokens),\n",
    "    \"questions_with_thinking\": len([t for t in all_generated_tokens if extract_thinking_process(t)]),\n",
    "    \"generations\": [\n",
    "        {\n",
    "            \"index\": i,\n",
    "            \"full_generation\": gen,\n",
    "            \"thinking_process\": extract_thinking_process(gen),\n",
    "            \"question\": meta[\"question\"],\n",
    "            \"answer\": meta[\"answer\"]\n",
    "        }\n",
    "        for i, (gen, meta) in enumerate(zip(all_generated_tokens, question_metadata))\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(output_summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved summary with all generated tokens to {output_summary_file}\")\n",
    "print(\"\\nDone!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
